### ***Project 3 Feedback***

***Nico Van de Bovenkamp***

***

**Overall:**  
Great work on this assignment! It's really good practice to run through the entire process. You built a solid model and took some time to work through the predictions that you got. You also did a very creative job at breaking down a quasi-regression problem into a classification problem! Overall, I think you should spend a bit of time understanding some fundamentals of the models and some fundamentals of the different parts/uses of our main API, Sklearn. What is the difference between `predict()`, `predict_proba()`, `predict_log_proba()`, and `decision_function()`? What is the precision_recall_cure actually calculating? How can I improve my model given these results / imbalance? I put some notes and comments below!

**Some Notes**  

* **Looping through values**  In general, it is not a good idea to loop through values in a for loop. You should take advantage of some pandas functions (and numpy) to make things speed up! I would recommend that you re-write that loop into a function and the apply it to each row: `df['Grade'] = df['G3'].apply(grade_bucketing)` where `grade_bucketing` is your function that checks a value and returns 1 if in range (0 - 5), etc.!
    - Check it out: https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6
* **Plotting**  When plotting with lots of features, I recommend that you take a look at just a subset of features. In particular, I would choose features of interest to be plotted by themselves and with your **target** variable. If you plot too much, you end up with an unreadable mess.
* **A note on predict methods**  In Sklearn, there are a few ways that we can "predict" in the api. The methods given are always defined within the documentation, as they are not always the same. For LogisticRegression, we have a `predict()`, `predict_proba()`, `predict_log_proba()`, and `decision_function()`. If you look through the documentation, you will notice that these functions output/mean different things. You have used the `decision_function()`, which outputs a "confidence score" by taking the distance of the given point from the hyper-plane (the plane that is separating our classes). A positive score indicates a high confidence via distance, and a low score would be greatly negative as it is on the "wrong side" of the prediction boundary. You can use this, but be aware of what it truly means! I would recommend using probabilities with Logistic Regression, as it's one of the benefits of having this model in the first place! `predict()` will predict the **class** and 'predict_proba()' will predict a probability.
* **Cross Validation**  In your notebook, you used the `cross_val_predict()` method to perform your cross validation. This is good, and can be quite useful if you want to see what kind of predictions your model could give you on a subset of your data. I would recommend you take more use of the `cross_val_score()` method. The scoring method allows us to see if your model is helping us evaluate the performance of our model on different subsets of the data in terms of accuracy, loss, precision, etc.
* **Precision Recall curve**  The above note should also give you some insight into why your precision_recall_cure function didn't really work. This is a case when a method will give you back *something*, but it doesn't really make sense. The scores you fed in are those distances from the decision boundary. Meanwhile, precision_recall_cure takes in probabilities. Why might that be? Well, what the precision recall curve is doing is it is taking a sweep of different cut-off points, or **thresholds**, and then showing you the precision score and recall score that you get from that threshold. **Also**, note that you may want to look at a precision recall curve at the top line (the average over each class), but you might also want to look at the percision_recall_curve of just ONE class. This will always be the case for AUC_ROC as it only computes the curve on single classes. This is why you get the error, "multiclass format not supported"
* **Hyperparameter optimization**  As we discussed in our review session, once you have decided on a solid set of features, you want to do some level of hyper parameter tuning. You currently aren't adding any penalty to your model, which could help with some over-fitting. It can also help with your class imbalance there.
    - Logistic regression:
        - C : Amount of penalty you apply to your weights! Remember that in this implementation, it is actually 1/C. Thus, a smaller C means more penalty as you are adding MORE of the weights to the penalty.
        - penalty: 'l1' vs. 'l2'
        - fit_intercept : (this is a parameter, but you likely always use this term)
        - solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} this is your term to change the optimization procedure! These can be interesting to experiment with. On a smaller dataset, this may not be so important, but when datasets become huge... This is very important.
